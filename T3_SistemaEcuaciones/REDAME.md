MÃ©todos Directos
Estos mÃ©todos encuentran la soluciÃ³n exacta (dentro de la precisiÃ³n numÃ©rica) en un nÃºmero finito de pasos. Se basan en transformar el sistema de ecuaciones para resolverlo directamente.

EliminaciÃ³n Gaussiana
DescripciÃ³n:
Convierte el sistema de ecuaciones en una matriz triangular superior usando operaciones elementales (como intercambiar filas, multiplicar por un escalar o sumar mÃºltiplos de filas). Luego, se resuelve el sistema por sustituciÃ³n hacia atrÃ¡s, empezando desde la Ãºltima ecuaciÃ³n hacia la primera.

FÃ³rmula:

latex
Copiar
Editar
$$
x_i = \frac{b_i - \sum_{j=1}^{i-1} a_{ij} x_j}{a_{ii}}, \quad i = n, n-1, \dots, 1
$$
Gauss-Jordan
DescripciÃ³n:
Es una extensiÃ³n del mÃ©todo de Gauss. No se detiene al obtener la forma triangular superior, sino que sigue transformando hasta que se obtiene la matriz identidad, lo que permite leer directamente las soluciones sin sustituciÃ³n.

FÃ³rmula:

$$
x_i = \frac{b_i - \sum_{j=1, j \ne i}^{n} a_{ij} x_j}{a_{ii}}
$$
MÃ©todos Iterativos
Estos mÃ©todos aproximan la soluciÃ³n comenzando con un valor inicial y mejorÃ¡ndolo en cada iteraciÃ³n. Se repiten hasta que la diferencia entre iteraciones es lo suficientemente pequeÃ±a.

Gauss-Seidel
DescripciÃ³n:
Usa los valores mÃ¡s recientes calculados dentro de la misma iteraciÃ³n. Esto hace que, en muchos casos, converja mÃ¡s rÃ¡pido que Jacobi.

FÃ³rmula:

$$
x_i^{(k+1)} = \frac{b_i - \sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \sum_{j=i+1}^{n} a_{ij} x_j^{(k)}}{a_{ii}}
$$
Jacobi
DescripciÃ³n:
Cada variable se actualiza usando Ãºnicamente los valores de la iteraciÃ³n anterior. No se usan los nuevos valores hasta que termine la iteraciÃ³n, lo que hace que sea mÃ¡s fÃ¡cil de implementar, pero puede requerir mÃ¡s iteraciones para converger.

FÃ³rmula:

$$
x_i^{(k+1)} = \frac{b_i - \sum_{j=1, j \ne i}^{n} a_{ij} x_j^{(k)}}{a_{ii}}
$$
MÃ©todo de la Secante
DescripciÃ³n:
Aproxima la derivada con dos puntos:

latex
Copiar
Editar
$$
x_{n+1} = x_n - f(x_n) \cdot \frac{x_n - x_{n-1}}{f(x_n) - f(x_{n-1})}
$$
Ventajas:

No necesita derivadas.

Mejora la convergencia frente al punto fijo.

Desventajas:

Requiere dos valores iniciales.

Puede fallar si 
ğ‘“
(
ğ‘¥
ğ‘›
)
=
ğ‘“
(
ğ‘¥
ğ‘›
âˆ’
1
)
f(x 
n
â€‹
 )=f(x 
nâˆ’1
â€‹
 ).

